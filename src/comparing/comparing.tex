Even though filtering test results can greatly reduce the amount of data that is involved in analyzing testing results, in order to detect performance regressions the results still have to be compared in some way to the results of a previous run of the test suite. The following are some of the problems that arise when doing so and some of the solutions that exist.

\section{Load dependency}
Performance metrics are dependent on the system load during execution of the test suite. This makes it difficult to straight out compare performance counters against results of previous iterations. Jiang et al present an automated way to abstract load testing data and analyze the behaviour of the system. \cite{jiang2010automated} Even though it is not intented to be used for performance regression testing specifically, it uses performance counters to compare system performance to predefined criteria and as such present a solution to the load dependency of performance counters.

\section{Comparing against historical data}
By normalizing and discretizing the performance counters and correlating the results against a historal dataset Foo et al are able to derive a performance signature \cite{foo2010mining} consisting of association rules between performance counters. An example of such a signature could be \{Arrival Rate = Medium, CPU utilization = Medium, Throughput = Medium\}. In combination with statistical techniques, these association rules enable analysts to automatically flag violations of rules in new tests and detect possible performance regressions with a certain confidence.

\section{Model based testing}
Model based testing leverages the advantage of being load-independent against the tasks of having to keep the model up-to-date. The load-independence also only goes as far as the different load for which the model was solved, which means it is not a viable method for detecting performance regressions when system load behaves in ways that was not anticipated in advance. Mi et al offer a way to derive a performance signature that is based on the latency of a transaction and the utilization of system resources, which are adjusted for the existence of outliers \cite{mi2008analysis}.

Ghaith et al build on the principle of load independent testing by using a queueing network to model the underlying system and using historical testing data to tune the model to the expected performance under various workloads. \cite{ghaith2013profile} This approach combines a transaction profile, a signature of the system resources utilized by some elementary action of the system, with historical testing data. A transaction profile can be obtained using statistical techniques for determing the service demands based on resource utilization \cite{casale2008robust} or response time \cite{kraft2009estimating}.

