Even though filtering test results can greatly reduce the amount of data that is involved in analyzing testing results, in order to detect performance regressions the results still have to be compared in some way to the results of a previous run of the test suite. The following are some of the problems that arise when doing so and some of the solutions that exist.

\section{Load-independent comparison}
Performance metrics are dependent on the system load during execution of the test suite. This makes it difficult to correctly identify performance regressions. Jiang et al present an automated way to abstract load testing data and analyze the behaviour of the system. \cite{jiang2010automated} Even though it is not intented to be used for performance regression testing specifically, it uses performance counters to compare system performance to predefined criteria and as such present a solution to the load dependency of performance counters.

Model based testing leverages the advantage of being load-independent against the tasks of having to keep the model up-to-date. The load-independence also only goes as far as the different load for which the model was solved, which means it is not a viable method for detecting performance regressions when system load behaves in ways that was not anticipated in advance. Mi et al offer a way to derive a performance signature that is based on the latency of a transaction and the utilization of system resources, which are adjusted for the existence of outliers \cite{mi2008analysis}.

\section{Detecting possible performance regressions}
In order to detect performance regressions in a system, a method of comparing the performance of different versions of the system is required. Since manual comparison is prone to errors and not viable for large amounts of performance metrics, the method should be able to automatically compare the performance by creating a profile for a version of the software and comparing it with the profile of previous verions. There are different ways to do this and they are likely to yield different results.

By normalizing and discretizing the performance counters and correlating the results against a historal dataset Foo et al are able to derive a perforance signature \cite{foo2010mining} consisting of association rules between performance counters. An example of such a signature could be \{Arrival Rate = Medium, CPU utilization = Medium, Throughput = Medium\}. In combination with statistical techniques, these association rules enable analysts to automatically flag violations of rules in new tests and detect possible performance regressions with a certain confidence.

\section{Tracing the cause of performance problems}
Tracing the cause of anomalies in performance can help in indentifying if something is a performance regression and what might have caused it. There are several approaches that aim to identify the underlying cause of performance regressions.

If the performance metrics measure system level activity, it is difficult to trace a performance regression back to the function call that causes it. By probing I/O write operations on application level, Bezemer et al are able to create a profile for the amount of I/O per function \cite{bezemer2014detecting}. This allows them to trace performance problems back to changes in specific function by comparing performance metrics with the version control system of the tested software. Not only does this provide the cause of the problem, it enables developers to use performance regression testing to guide optimizing performance.

Ghaith et al build on the principle of load-independent and model based testing by using a queueing network to model the underlying system and using historical testing data to tune the model to the expected performance under various workloads. \cite{ghaith2013profile} This approach combines a transaction profile, a signature of the system resources utilized by some elementary action of the system, with historical testing data. A transaction profile can be obtained using statistical techniques for determing the service demands based on resource utilization \cite{casale2008robust} or response time \cite{kraft2009estimating}.
