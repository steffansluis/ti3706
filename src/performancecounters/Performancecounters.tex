\section{Performance counters}

This chapter discusses how perfomance counters can give information about the quality of software. Performance counters are a type of data output as a result of performance regression testing. Performance counters can count different types of events like CPU utilization, memory utilization or disk IO.

\subsection{Results of performance counters}
To detect if performance regression occurs, the results of the prior and the new performance regression test need to be compared. "A performance regression means that the new version uses more resources or has less throughput than prior versions" \cite{DetectionPerformanceRegression}. The results of these tests can be analyzed manually, but this costs a lot of time. A faster approach would be to use a control chart. A control chart consists of a baseline data set and a target data set. A baseline data set contains the results of the prior test. Based on this data set an upper and lower limit will be decided. The target data set contains the results of the new test. From these two data sets the violation ratio can be calculated. This is the percentage of the amount of targets outside the upper and lower limit. If performance regression occurs, the violation ratio is too high. It now seems easy to detect if performance regression occurs: a certain threshold on the maximum allowed violation ratio can be chosen, and if the violation ratio is higher than this threshold performance regression has occurred. Unfortunately, this is not the case. There are some things that make it hard to detect performance regression. If the violation ratio is low, the probability that performance regression has occurred is low as well. A high violation ratio doesn't always mean that performance regression has occurred. Some performance counters are inconsistent, and show different results on the same input data. Because of this, it is possible that the violation ratio is higher than the chosen threshold but no performance regression has occurred. To avoid this, the prior test will be executed again after the new test.

\subsection{Accuracy of performance counters}
But how accurate are the results of performance counters? What techniques can make these results more accurate?

Experiments have shown that "reducing the number of concurrently measured hardware events can be a good way to improve measurement accuracy"\cite{AccuracyPerformanceCounter}. However, this depends on some factors, for instance the type of interface that is used, or the counter configuration.
Tests with different interfaces (perfmon2 and perfctr) and different counter configurations (user and user+kernel) show that for the combination of the perfmon2 interface and user+kernel configuration the measurement error increases as the number of registers increases as well. \cite{AccuracyPerformanceCounter}
Also, the type of infrastructure is very dependent. The measurement error reduces a lot when a low-level infrastructure is used, instead of a high-level infrastructure.
On top of that, for the user+kernel mode the measurement error gets bigger if the duration of the benchmark gets longer. \cite{AccuracyPerformanceCounter} So to make the measurement error smaller, less loop iterations should be used.  The infrastructure doesn't have influence. For the user mode the duration of the benchmark doesn't matter. This is because of interrupts, that only occur in kernel event counts. More interrupt-related instructions will include if the duration of a measurement takes longer.

