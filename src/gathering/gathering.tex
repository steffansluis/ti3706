Once the testing process has been organised to facilitate the particular software development method that is being used, the testing can finally begin. The next challenge that presents itself is what to test for. This chapter discusess what information is relevant to the performance of software and how to handle that information in order to obtain the most accurate representation of the underlying performance.

\section{Types of data: performance counters}
Performance counters are used to measure events that occur duing program execution\cite{PC}. Examples are: the number of instructions, loads, bandwidth or the number of executed cycles.

``The goal of analyzing performance counters is to detect if a performance regression has occured, where the performance regression has occured, and what causes the regression.'' \cite{nguyen2012using}

To detect if performance regressions occur, two different tests will be run: a target run and a baseline run. The target run is the new test run, where the baseline run is a good past run. On both tests equivalent performance counters can be used as input to detect if performance regressiond have occured by comparing the output of the tests. Comparing the output of the tests is a big challenge, especially in big complex software systems.

If it is known that a performance regression has occured, the next problem is to detect where the performance regression has appeared. Big software systems consist of many types of components, and each component may have many instances. Detecting where the perfomance regressions have shown up is very difficult, because there are many performance counters for each instance of each component. This can be very time consuming.

\section{Accuracy and meaning of performance metrics}
After determining the location of the performance regressions, the reason of the performance regressions needs to be determined. ``Understanding the kind of problem usually requires static analysis of source code.'' \cite{nguyen2012using}

The fact that performance counters are often used by application developers shows that performance counters are very useful. However, performance counters are not that accurate. Tests with different interfaces (perfmon2 and perfctr) and different counter configurations (user and user+kernel) show that for the combination of the perfmon2 interface and user+kernel configuration the measurement error increases when the number of registers increases. \cite{AccuracyPerformanceCounter}
Also, the type of infrastructure is very dependent. The measurement error reduces a lot when a low-level infrastructure is used, instead of a high-level infrastructure.
On top of that, for the user+kernel mode the measurement error gets bigger if the duration of the benchmark gets longer. \cite{AccuracyPerformanceCounter} So to make the measurement error smaller, less loop iterations should be used.  The infrastructure does not have influence. For the user mode the duration of the benchmark does not matter. This is because of interrupts, that only occur in kernel event counts. More interrupt-related instructions will include if the duration of a measurement takes longer.
